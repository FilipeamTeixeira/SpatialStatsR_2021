#### Spatial Data Analysis in R - Day 4 ####
#### Spatial Statistics - part II ####

library(spatstat)

# Spatstats works with ppp objects, not SpatialPolygonsDataframes
# A ppp object may or may not have attribute information
# (also referred to as marks)

# The Data #
load("data/starbucks.RData")

# ma - window object with Massachusetts border
# starbucks - point (ppp) object with locations of starbucks
# pop - raster image with population density

# We will only concern ourselves with the pattern generated by the points
# and not their attributes, so we make sure that the marks are NULL.

marks(starbucks)  <- NULL

# We need to define the borders of the starbucks points - or in this case Window

Window(starbucks) <- ma

# Transforming the skewed distribution in the population density
# covariate may help reveal
# relationships between point distributions and the covariate
# in some of the point pattern analyses

# It is better to work with normal distributions in some of the
# point pattern analysis

hist(pop) # awfully skewed

pop_log <- log(pop)
hist(pop_log) # better

# 11.a
#### Density Based Analysis ####

# 11.a.i
# Quadrat Density

quad_count <- quadratcount(starbucks, nx= 6, ny=3)

plot(starbucks, pch=20, cols="#1DACE8", main="Quadrat Count")  # Plot points
plot(quad_count, add=TRUE, col="#76A08A", lwd=.3)  # Add quadrat grid

# Calculate Density
quad_dens <- intensity(quad_count, image = TRUE)

wes <- colorRampPalette(c("#1DACE8", "#1C366B")) # set nicer colors

plot(quad_dens, main="Quadrat Density", col = wes, las = 1)  # Plot density raster
plot(starbucks, pch=19, cex=0.4, col=rgb(0,0,0,.5), add=TRUE)  # Add points

# The numbers look messy as we're seeing the number of stores/m2
# Lets rescale to km

starbucks_km <- rescale(starbucks, 1000, "km")
ma_km <- rescale(ma, 1000, "km")
pop_km    <- rescale(pop, 1000, "km")
pop_log_km <- rescale(pop_log, 1000, "km")

# Calculate density for km2

quad_count_km <- quadratcount(starbucks_km, nx= 6, ny=3)
quad_dens_km <- intensity(quad_count_km, image = TRUE)

plot(quad_dens_km, main="Quadrat Density km2", las=1)  # Plot density raster
plot(starbucks_km, pch=19, cex=0.4, col=rgb(0,0,0,.5), add=TRUE)  # Add points


# 11.a.ii
# Kernel Density
# Similar to Quadrat Density, but instead, the sub-regions overlap,
# providing a transition area (sub-region window).

kern <- density(starbucks_km)
plot(kern, main = "Kernel Density", las=1)
contour(kern, add=TRUE)

# Set a higher bandwidth/range (50km)
kern_50km <- density(starbucks_km, sigma=50)

plot(kern_50km, main = "Kernel Density", las=1)
contour(kern_50km, add=TRUE)

# Lets change the type of smoothing function to epanechnikov

kern_50km_ep <- density(starbucks_km, sigma=50, kernel = "epanechnikov")
plot(kern_50km_ep, main = "Kernel Density", las=1)
contour(kern_50km_ep, add=TRUE)


# 11.b
#### Distance based analysis ####

# 11.b.i
# Average nearest neighbor analysis
# For the 1st neighbor - k=1
mean(nndist(starbucks_km, k=1)) # For the 1st neighbor - k=1
mean(nndist(starbucks_km, k=2)) # For the 2nd neighbor - k=2

mean_nn <- apply(nndist(starbucks_km, k=1:100), 2, FUN = mean)

# NOTE: apply function runs function (in this case mean), for a matrix or array
# 2 in apply(), means it will calculate function for columns
# k = 1:100 will calculate for the first 100 neighbors

plot(mean_nn ~ eval(1:100), type="b", main=NULL,
     las=1, pch = 19, cex = .5)

# NOTE: eval() - Evaluates an expression (in this case a sequence)
# seq() works as well


#11.b.ii
#### K and L functions ####
# K function
help(Kest)

# The estimation of K is hampered by edge effects arising
# from the unobservability of points of the random pattern outside the window.
# An edge correction is needed to reduce bias (Baddeley, 1998; Ripley, 1988).
# The corrections implemented here are:
# border, isotropic, translate, rigid, none, best, good..

# NOTE:  The line (Kpois) represents the theoretical K function
# under the null hypothesis that the points are completely randomly distributed.
# Basically Kpois - theoretical Poisson K(r)

kfun <- Kest(starbucks_km, correction = "iso")
plot(kfun, main="K-Function", las=1)

# NOTE: if your plot margins look funky, check here how to change them:
# https://bookdown.org/ndphillips/YaRrr/plot-margins.html


# K greater (above) than Kpois(r) indicate clustering of points
# at a given distance band.

# K lower (below) than Kpos(r) indicate dispersion of points
# at a given distance band.

# L Function
lfun <- Lest(starbucks_km, correction = "iso")
plot(lfun, main="L-Function", las=1)

# G Function
gfun <- pcf(starbucks_km)
plot(gfun, main = "G-Function")


# 11.b

#### Clustering ####
library(tidyverse)
library(geosphere)
library(sf)

# Import and filter data
unzip("data/crime/crime.zip", exdir = "data/crime/")
crime <- data.table::fread("data/crime/crime.csv")
denver <- rgdal::readOGR("data/crime/denver.shp")

# Filter by offense
crime <- crime %>%
  mutate(GEO_LAT = as.numeric(GEO_LAT),
         GEO_LON = as.numeric(GEO_LON)) %>%
  filter(!is.na(GEO_LAT),
         OFFENSE_TYPE_ID == "drug-marijuana-possess")

# Convert to SpatialPointsDataFrame
crime_sp <- SpatialPointsDataFrame(crime[,c('GEO_LON', 'GEO_LAT')], crime,
                                proj4string = CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"))

plot(denver, border=gray(.5))
plot(crime_sp, pch = 19, cex = .4, col = "#1DACE8", add = TRUE)


crime_dist <- distm(crime_sp) #create distance matrix

crime_hc <- hclust(as.dist(crime_dist), method="complete") #create cluster

crime_sp$clust <- cutree(crime_hc, h = 5000) #cutoff of 5km
#NOTE: Try to play with the h value. Increase it to 10000

plot(denver, border=gray(.5))
plot(crime_sp, col = crime_sp$clust,  pch = 19, cex = .4, add = TRUE)

# If margins are too big or wonky, run par(mar = c(0,0,0,0))
# this sets the margins to 0


